---
title: 逻辑回归
toc: true
date: 2018-09-20 11:05:12
tags: 逻辑回归
categories: 面试
---

# 基础篇

**1. 请写一下LR的cost function和梯度下降参数迭代公式**

---

$$
\begin{aligned}
P(Y=1|x) &= \frac{\exp(\beta^Tx)}{1+\exp(\beta^Tx)} = \sigma(x) \\
P(Y=0|x) &= \frac{1}{1+\exp(\beta^Tx)} = 1 - \sigma(x) \\
l(\beta) &= -\ln L(\beta) \\
	   &= -\sum_{i=1}^{m} (y_i \ln(\sigma(x_i)) + (1 - y_i) \ln(1-\sigma(x_i))) \\
	   &= -\sum_{i=1}^{m} (y_i (\beta^Tx_i) - \ln (1+\exp(\beta^Tx_i))) \\
	   &= \sum_{i=1}^{m} (\ln (1+\exp(\beta^Tx_i)) - y_i (\beta^Tx_i)) \\
\frac{\partial l}{\partial \beta} &= \sum_{i=1}^m ((\sigma(x_i) - y_i)x_i) \\
\beta &= \beta + \sum_{i=1}^m (x_i(y_i - \sigma(x_i))) \\
\end{aligned}
$$

为什么可以用似然函数？答：因为目标是要让预测为正的的概率最大，且预测为负的概率也最大，即每一个样本预测都要得到最大的概率，将所有的样本预测后的概率进行相乘都最大，这就能到似然函数了

**2. 怎么防止过拟合**

---

通过正则化方法。正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等。

**3. 为什么正则化可以防止过拟合？**

---

过拟合表现在训练数据集上的误差非常小，而测试数据集上的误差反而增大。其原因一般是由于模型过于复杂， 过分拟合数据的噪声。正则化是对模型参数添加先验条件，使得模型复杂度较小，对于噪声的扰动较小。

比如抛硬币，五次都是向上，可能得到的结果是向上的概率为1，但是这是严重过拟合的。如果加上先验0.5，抛硬币的结果就不会再那么离谱。

**4. L1正则化与L2正则化有什么区别？**

---

首先都是用于避免过拟合。

不同点：**L2与L1区别在于，L1是拉普拉斯先验，L2是高斯先验。**

L1计算的是参数的1范数，即绝对值的和，L2计算的是参数的2范数，即各项平方和。

L1可以产生稀疏解，可以让一部分特征的系数缩小到0，从而间接的实现特征的选择。所以L1适用于特征之间有关联的情况。

L2是让所有特征的系数缩小，但不会减少为0，它会使得优化求解稳定快速。适用于特征之间没有关联的情况。

由于L1服从拉普拉斯分布，所以L1在0处不可导，难以计算，这个方法可以用Proximal Algorithms或ADMM来解决。

![v2-f2f9e95b70a7ce926beed810e80045a9_hd](https://ws3.sinaimg.cn/large/006tNbRwgy1fvh7q29260j30gw0aq0t4.jpg)

上图中的模型是线性回归，有两个特征，要优化的参数分别是w1和w2，左图的正则化是l2，右图是l1。蓝色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值（假设一个样本），半径就是误差值，受限条件就是红色边界（就是正则化那部分），二者相交处，才是最优参数。可见右边的最优参数只可能在坐标轴上，所以就会出现0权重参数，使得模型稀疏。

**5. L1正则化不可导，怎么求解？**

---

坐标轴下降法（按照每个坐标轴一个一个使其收敛）

最小角回归（是一个逐步的过程，每一步都选择一个相关性很大的特征，总的运算步数只和特征数目有关，和训练大小无关）

**6. 逻辑回归为什么一般性能差？**

---

LR是线性的，不能得到非线性关系，实际问题并不完全能用线性关系就能拟合。

**7. 如何用LR解决非线性问题？**

---

将特征离散成高维的特征可以解决分类模型的非线性问题

# 高级篇

**1. 为什么逻辑回归比线性回归要好？**

---

逻辑回归和线性回归首先都是广义的线性回归，

其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，

另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好

**2. 逻辑回归与最大熵模型MaxEnt的关系?**

---

逻辑回归跟最大熵模型**没有本质区别**。逻辑回归是最大熵对应类别为**二类**时的特殊情况，也就是当逻辑回归类别扩展到**多类**别时，就是最大熵模型。

**3. LR参数求解的优化方法**

---

梯度下降法，随机梯度下降法，牛顿法，LBFGS，BFGS,OWLQN

**4. 工程上，怎么实现LR的并行化？有哪些并行化的工具**

逻辑回归的并行化最主要的就是对**目标函数梯度计算的并行化**。

算法的并行化有两种

1. 无损的并行化：算法天然可以并行，并行只是提高了计算的速度和解决问题的规模，但和正常执行的结果是一样的。
2. 有损的并行化：算法本身不是天然并行的，需要对算法做一些近似来实现并行化，这样并行化之后的双方和正常执行的结果并不一致，但是相似的。

基于Batch的算法(Batch-GD, LBFGS, OWLQN)都是可以进行无损的并行化的。而基于SGD的算法（Ad Predictor， FTRL－Proximal）都只能进行有损的并行化。

并行化的工具**MPI和OpenMP**

# 附录

## L1正则化求导方法

### 线性回归

线性回归一般形式：$h(X) = X\theta$

需要极小化的函数为：$J(\theta) = \frac{1}{2} (X\theta - Y)^T(X\theta -Y)$

梯度下降求解，每一轮迭代$\theta$表达式为：$\theta = \theta - \alpha X^T (X\theta - Y)$

最小二乘法，结果为：$\theta = (X^T X)^{-1} X^T Y$

### Ridge回归

损失函数为：$J(\theta) = \frac{1}{2} (X\theta - Y)^T (X\theta--Y) + \frac{1}{2} \alpha ||\theta||^2_2$

每一轮$\theta $迭代表达式为：$\theta = \theta - (\beta X^T (X\theta - Y) + \alpha \theta)$，$\beta$ 为步长

最小二乘法，结果为：$\theta = (X^TX + \alpha I)^{-1} X^T Y$

Ridge回归在不抛弃任何一个变量的情况下，缩小的回归系数，使得模型相对而言比较稳定，但是模型的变量很多，模型解释性差。

## Lasso回归

损失函数为：$J(\theta) = \frac{1}{2} (X\theta - Y)^T (X\theta--Y) + \frac{1}{2} \alpha ||\theta||_1$

由于$||\theta||_1$在0处不存在导数，所以需要借助坐标下降法或最小角回归法进行计算。

#### 坐标下降法

坐标下降法的原理为：一个可微的凸函数$J(\theta)$，其中$\theta$是$n\times 1$的向量。如果在某一点$\bar{\theta}$， 是的$J(\theta)$在每一个坐标轴$\bar{\theta}(i=1,2,\cdots, n)$都是最小值，则$J(\bar{\theta})$就是一个全局的最小值。

- 随机为$\theta$向量随机初始化一个值，为$\theta^{(0)}$，括号中的数字代表迭代次数。

- 对于第k轮迭代，有
  $$
  \theta^{(k)}_i \in argmin J(\theta_1^{(k)}, \theta_2^{(k)}, \cdots, \theta_{i-1}^{(k)}, \theta_i, \theta_{i+1}^{(k-1)}, \cdots, \theta_n^{(k-1)})
  $$
  只有$\theta_i$ 为变量，其余为常量

- 检查$\theta^{(k)}$与$\theta^{(k-1)}$向量在各个维度上的变化情况，如果在所有维度变化都足够小，则$\theta^{(k)}$为最终结果，否则转入步骤2，继续k+1轮迭代

### 最小角回归法

# 参考文献

1. [Logistic Regression常见面试题整理](https://zhuanlan.zhihu.com/p/34670728)