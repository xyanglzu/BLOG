---
title: 机器学习算法工程师面经
toc: true
date: 2018-03-27 16:17:27
tags:
categories: 机器学习算法面试

---

1. 介绍LR（Logistic Regression）

   ## 定义：

   - 二元LR模型是用来估计一个二元响应变量的概率值。

   - 二项LR模型是一下的条件概率分布：

     $$P(Y=1|x) = \frac{exp(w \cdot x + b)}{1 +exp(w \cdot x + b) }$$

     $$P(Y=0 | x) = \frac{1}{1 +exp(w \cdot x + b) }$$

   - 对于给定的输入实例$x$，按照上式可得到对应的条件概率，比较两个条件概率大小，将实例分到较大的一类。

   ## 损失函数

   - 应用**极大似然估计法**估计模型参数，从而将问题转化为以对数似然函数为目标函数的最优化问题，然后采用梯度下降法或者拟牛顿法进行求解
   - 似然函数：$\prod_{i=1}^N [\pi(x_i)]^{y_i} [1-\pi(x_i)]^{1-y_i}$
   - 对数似然函数：$\sum_{i=1}^N [y_i \log(\pi (x_i)) + (1-y_i)\log(1-\pi(x_i))]$
   - 参数更新公式：$w^{(i)} = w^{(i)} - \alpha \sum_i^n (\pi(x^{(i)}) - y^{(i)})x^{(i)}$

   ## **优点**：

   - LR模型直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确带来的问题
   - LR模型不是仅仅预测出类别，而是可得到近似概率预测，这对于许多须利用概率辅助决策的任务很有用
   - 对数几率函数是任意阶可导凸函数，容易取得最优解

   ## 多项LR模型

   - 离散型随机变量Y的取值集合为$\{1, 2, \dots, K\}$

     $P(Y=k|x) = \frac{exp(w_k \cdot x)}{1 + \sum_{k=1}^{K-1} exp(w_k \cdot x)}$

     $P(Y=K|x) = \frac{1}{1 + \sum_{k=1}^{K-1} exp(w_k \cdot x)}$

2. LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？

   - 逻辑回归的条件分布式伯努利分布，而线性回归是高斯分布
   - 逻辑回归要预测的是概率，因此要通过逻辑分布函数约束到（0，1）区间
   - 线性回归是连续型模型，用于分类的问题时受噪声影响比较大，logistic回归是非线性模型（上文），本质上是线性回归模型，但logistic回归巧妙之处在于其将结果值经一层函数映射在0-1上，即在特征到结果的映射中加入了一层函数映射（对数损失函数），也就是本文的sigmoid function。

3. SVM原问题和对偶问题关系

   - 原始问题：
     $$
     \begin{align} 
     \min_{w, b, \xi_i}  \quad &  \frac{1}{2} ||w|^2 + C \sum_{i=1}^N \xi_i  && \\
      s.t. \quad& y_i (w \cdot x_i + b) \ge 1 - \xi_i, & i = 1,2,\cdots, N  \\
      &\xi_i, \ge 0 \quad & i = 1,2,\cdots, N &
     \end{align}
     $$

   - 拉格朗日函数：
     $$
     L(w, b, \xi, \alpha, \mu) = \frac{1}{2}||w||^2 + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N \alpha_i (y_i (w \cdot x_i + b) - 1 + \xi_i) - \sum_{i=1}^N \mu_i \xi_i
     $$

   - KKT条件：
     $$

     $$
     ​