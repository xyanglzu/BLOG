---
title: 机器学习算法工程师面经
toc: true
date: 2018-03-27 16:17:27
tags:
categories: 机器学习算法面试

---

1. 介绍LR（Logistic Regression）

   ## 定义：

   - 二元LR模型是用来估计一个二元响应变量的概率值。

   - 二项LR模型是一下的条件概率分布：

     $$P(Y=1|x) = \frac{exp(w \cdot x + b)}{1 +exp(w \cdot x + b) }$$

     $$P(Y=0 | x) = \frac{1}{1 +exp(w \cdot x + b) }$$

   - 对于给定的输入实例$x$，按照上式可得到对应的条件概率，比较两个条件概率大小，将实例分到较大的一类。

   ## 损失函数

   - 应用**极大似然估计法**估计模型参数，从而将问题转化为以对数似然函数为目标函数的最优化问题，然后采用梯度下降法或者拟牛顿法进行求解
   - 似然函数：$\prod_{i=1}^N [\pi(x_i)]^{y_i} [1-\pi(x_i)]^{1-y_i}$
   - 对数似然函数：$\sum_{i=1}^N [y_i \log(\pi (x_i)) + (1-y_i)\log(1-\pi(x_i))]$
   - 参数更新公式：$w^{(i)} = w^{(i)} - \alpha \sum_i^n (\pi(x^{(i)}) - y^{(i)})x^{(i)}$

   ## **优点**：

   - LR模型直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确带来的问题
   - LR模型不是仅仅预测出类别，而是可得到近似概率预测，这对于许多须利用概率辅助决策的任务很有用
   - 对数几率函数是任意阶可导凸函数，容易取得最优解

   ## 多项LR模型

   - 离散型随机变量Y的取值集合为$\{1, 2, \dots, K\}$

     $P(Y=k|x) = \frac{exp(w_k \cdot x)}{1 + \sum_{k=1}^{K-1} exp(w_k \cdot x)}$

     $P(Y=K|x) = \frac{1}{1 + \sum_{k=1}^{K-1} exp(w_k \cdot x)}$

2. LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？

   - 逻辑回归的条件分布式伯努利分布，而线性回归是高斯分布
   - 逻辑回归要预测的是概率，因此要通过逻辑分布函数约束到（0，1）区间
   - 线性回归是连续型模型，用于分类的问题时受噪声影响比较大，logistic回归是非线性模型（上文），本质上是线性回归模型，但logistic回归巧妙之处在于其将结果值经一层函数映射在0-1上，即在特征到结果的映射中加入了一层函数映射（对数损失函数），也就是本文的sigmoid function。

3. SVM原问题和对偶问题关系

   - 原始问题：
     $$
     \begin{align} 
     \min_{w, b, \xi_i}  \quad &  \frac{1}{2} ||w|^2 + C \sum_{i=1}^N \xi_i  && \\
      s.t. \quad& y_i (w \cdot x_i + b) \ge 1 - \xi_i, & i = 1,2,\cdots, N  \\
      &\xi_i, \ge 0 \quad & i = 1,2,\cdots, N & \\
     \end{align}
     $$

   - 拉格朗日函数：
     $$
     L(w, b, \xi, \alpha, \mu) = \frac{1}{2}||w||^2 + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N \alpha_i (y_i (w \cdot x_i + b) - 1 + \xi_i) - \sum_{i=1}^N \mu_i \xi_i
     $$

   - KKT条件：
     $$

     $$

4. L1正则为什么可以把系数压缩成0？

   - L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。但是实现稀疏大家都是使用L1范数，为什么？这是因为L0范数很难求导

   -  L1范数是指向量中各个元素绝对值之和。它是L0范数的最优凸近似，称为“稀疏规则算子”

   - **为什么使参数稀疏？**

     - 特征选择：一般来说$x_i$的大部分特征与输出$y_i$无关，经过参数稀疏，可以将无关的特征去掉，完成自动选择特征的任务。
     - 可解释性：训练出的模型容易解释，使得输出只和部分特征有关，更加直观。

   -  L1在江湖上人称Lasso，L2人称Ridge。L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

   - L1,L2范式来自于对数据的先验知识.如果你认为,你现有的数据来自于高斯分布,那么就应该在代价函数中加入数据先验P(x),一般由于推导和计算方便会加入对数似然,也就是log(P(x)),然后再去优化,这样最终的结果是,由于你的模型参数考虑了数据先验,模型效果当然就更好，取对数后的log(P(x))就剩下一个平方项了,这就是L2范式的由来--高斯先验.
     同样,如果你认为你的数据是稀疏的,不妨就认为它来自某Laplace分布。laplace分布的概率密度函数为：

     ![Laplace](/var/folders/xm/bv9q0c8j0tl753vytq1g18780000gn/T/abnerworks.Typora/image-201803291510257.png)

     对Laplace概率密度函数求对数，得到的就是一个绝对值，就是L1范数。

5. 坐标下降法的具体实现细节

   ![mage-20180329152131](/var/folders/xm/bv9q0c8j0tl753vytq1g18780000gn/T/abnerworks.Typora/image-201803291521316.png)

6. ​